{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95c4afd61e374d3cacfd51baeed2712d",
      "dc8791a95381458c9e2a8862a56e4405",
      "664f7af0becd45118a7cdb4fec37739b",
      "b77b6587a122455b87de382f1e1a934f",
      "f416616e044a4d788072153e47b3e0d2",
      "c33643a14c904107ac956868ad6bea52"
     ]
    },
    "id": "9BxeCvi8gGvS",
    "outputId": "86d553a8-837c-4f0e-f04d-b215aeee056d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextDataset,DataCollatorForLanguageModeling, Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocessing(path):\n",
    "\twith open(path, 'r') as f:\n",
    "\t\ttext = f.read().replace('\"',\"\").replace(\"«\",\"\").replace(\"«\",\"»\").replace(\"‘\",\"\").split('\\n\\n')\n",
    "\ttext = list(map(lambda x: \"<BOS>\"+x.replace(\"\\n\",\" \"), text))\n",
    "\treturn text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-italian-embeddings\", max_lenght=256)\n",
    "model = AutoModelWithLMHead.from_pretrained(\"GroNLP/gpt2-small-italian-embeddings\")\n",
    "\n",
    "input_data = \"divina_commedia.txt\"\n",
    "\n",
    "text = preprocessing(input_data)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train, eval = train_test_split(text, train_size=.9, random_state=2020)\n",
    "\n",
    "with open('train_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<EOS>\".join(train))\n",
    "\n",
    "with open('eval_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<EOS>\".join(eval))\n",
    "\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset('train_tmp.txt','eval_tmp.txt',tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dantebert\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=200, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=1500, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "c-rUMyCom2PI",
    "outputId": "88043234-c8d5-4042-95f3-3b50d3012aaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./dantebert/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"GroNLP/gpt2-small-italian-embeddings\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100,\n",
      "      \"no_repeat_ngram_size\": 4,\n",
      "      \"num_beams\": 10,\n",
      "      \"repetition_penalty\": 10.0,\n",
      "      \"temperature\": 2.0,\n",
      "      \"top_k\": 20,\n",
      "      \"top_p\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30003\n",
      "}\n",
      "\n",
      "loading configuration file ./dantebert/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"GroNLP/gpt2-small-italian-embeddings\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100,\n",
      "      \"no_repeat_ngram_size\": 4,\n",
      "      \"num_beams\": 10,\n",
      "      \"repetition_penalty\": 10.0,\n",
      "      \"temperature\": 2.0,\n",
      "      \"top_k\": 20,\n",
      "      \"top_p\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30003\n",
      "}\n",
      "\n",
      "loading weights file ./dantebert/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./dantebert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100,\n",
      "      \"no_repeat_ngram_size\": 4,\n",
      "      \"num_beams\": 10,\n",
      "      \"repetition_penalty\": 10.0,\n",
      "      \"temperature\": 2.0,\n",
      "      \"top_k\": 20,\n",
      "      \"top_p\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/61d34f8dfb7c3ffe977154bdb55f9c4710693f1b6a1906e9bb5137b3169cfce6.79a3bbc906c703094307f2010d6cb2883ae928674a9884228651b26ca388437c\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/4efdb93b73c15e8a27f37fc1da3f9f94699064cf159205af5872e0ec0213489e.6227370d56d17082437cb8711626b11e99ec5bbcc0c8e17f5d1760a61030894f\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/f86edc23a3474e3a7190f4522ce04a3b10f3a170e310221a62091f7ec217eaf7.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
      "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/99f134cdd73cf165da27bb044dfbe8335fcc03aba30faeb9096ee0f4b531758e.3966d29321e63095f25361ba9a09e3db3ca4a3260118c28f30ff62177986e6be\n",
      "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100,\n",
      "      \"no_repeat_ngram_size\": 4,\n",
      "      \"num_beams\": 10,\n",
      "      \"repetition_penalty\": 10.0,\n",
      "      \"temperature\": 2.0,\n",
      "      \"top_k\": 20,\n",
      "      \"top_p\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30001\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100,\n",
      "      \"no_repeat_ngram_size\": 4,\n",
      "      \"num_beams\": 10,\n",
      "      \"repetition_penalty\": 10.0,\n",
      "      \"temperature\": 2.0,\n",
      "      \"top_k\": 20,\n",
      "      \"top_p\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30001\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'E colei che amai uopo più si vuoli, d’altrui lume già bianche e fioche, quasi come piante novelle rinognate per l’acqua fosser pronte.e disse: Piglia quel seme a li occhi; volgi ’l viso, e fammi nota la larghezza di questa nutrice».S’el s’aunasse ancor tutta la gente che già, in su la fortunata terra di Puglia, fu del suo sangue'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "dante = pipeline('text-generation',model='./dantebert', tokenizer='GroNLP/gpt2-small-italian-embeddings')\n",
    "dante(\"E colei che amai \")[0]['generated_text']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dantebert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
