{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmcliAKynVmm"
      },
      "source": [
        "# Finetune qwen model using unsloth\n",
        "The final model should talk as Dante Alighieri would do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "7a8a2932f0c14f9b9fac55d3c4c20163",
            "e767b4128cef43e8b2991fa49380b5b9",
            "5a04dcc63cba40d79550749d48d32f3c",
            "900112bb27b24f87806fafa7dbc3a90e",
            "da150acc837c40aeb286f4d5bb21e38f",
            "747a6752b0e24ae99a1e12946a3e15bb",
            "eb4d9cd0991f454fadc1d06040a39205",
            "152ce8ba9d284952b45c5d99e8dde5f9",
            "09191f4251f743f0a01631fa8ec99272",
            "8861b6da71b546adb5fb85c63bcc0fea",
            "3d3857537c5b4d6892d6e6536d261f56"
          ]
        },
        "id": "VXKzafF0nVmp",
        "outputId": "b62a8ecf-7898-4818-b5ff-9e150a235c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.3: Fast Qwen3 patching. Transformers: 4.53.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a8a2932f0c14f9b9fac55d3c4c20163",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0uB58nTnVmq",
        "outputId": "6f3ab14c-8ce4-41ac-b5bf-a31f7f73ebbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.7.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Add LoRA adapter\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ebd700808e2545b59bd0174899c5b91c",
            "7ca4df8d0bfd481ab00aba17768ecfbb",
            "05616a034835476eb9eff1fcf8ef3459",
            "741b029ace854e5c8c5c39e4d989e4da",
            "f141242baafb49bb9cd08a227872c148",
            "ef6d2b2d613841568183a6566ef96e8b",
            "640fe00b0198404ab96d314dd1c1b474",
            "cc93294227f64bfb9e50c3858c6b290e",
            "a16412374f684ee087a074b8cd91ed5c",
            "11e94b0d95db4abd9df2026d3501bf07",
            "3c360f59d6be4dcab142deab90344540"
          ]
        },
        "id": "1cBN6sKYnVms",
        "outputId": "5d536be6-cdab-42e5-b2cd-06a651601a9b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebd700808e2545b59bd0174899c5b91c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/654 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"../data/*.jsonl\", split=\"train\")\n",
        "\n",
        "def standardize_data_formats_custom(example):\n",
        "  instruction = {'content': example[\"instruction\"], 'role': 'user'}\n",
        "  response = {'content': example[\"response\"], 'role': 'assistant'}\n",
        "  return {\"conversations\":[instruction, response], }\n",
        "\n",
        "original_columns = dataset.column_names\n",
        "dataset = dataset.map(standardize_data_formats_custom, remove_columns=original_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDDhzLVEtYvM",
        "outputId": "3a0f2878-f73c-4777-e831-48b8bbec4c5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conversations': [{'content': 'How do I deal with stress?', 'role': 'user'},\n",
              "  {'content': \"Se l'affanno ti preme e fa dimora / nel petto tuo, come fiera che rugge, / non cercar fuga in ciÃ² che ancor devora. / Pianta lo piÃ¨ fermo, e con mente che fugge / il vano e lasso mondo, contempla il foco / che purga l'alma e il cor saldo riluce. / Medita il ciel, e alzati poco a poco / da l'onda che ti sbatte senza riva; / chÃ© sol chi spera in Dio, mai non Ã¨ fioco.\",\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M9oHLmlytkY9",
        "outputId": "0044fe91-cd86-4480-fdc1-0775ae3e8634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'content': 'How do I deal with stress?', 'role': 'user'}, {'content': \"Se l'affanno ti preme e fa dimora / nel petto tuo, come fiera che rugge, / non cercar fuga in ciÃ² che ancor devora. / Pianta lo piÃ¨ fermo, e con mente che fugge / il vano e lasso mondo, contempla il foco / che purga l'alma e il cor saldo riluce. / Medita il ciel, e alzati poco a poco / da l'onda che ti sbatte senza riva; / chÃ© sol chi spera in Dio, mai non Ã¨ fioco.\", 'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0])\n",
        "dataset_training = pd.DataFrame(dataset)\n",
        "dataset_training['text'] = dataset_training['conversations'].apply(\n",
        "    lambda conv: tokenizer.apply_chat_template(\n",
        "        conv,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        ")\n",
        "text_dataset = Dataset.from_pandas(dataset_training[['text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrJBvM68aA2Q",
        "outputId": "e60fa2eb-bad5-40d1-ff09-4d573aa77319"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 654\n",
              "})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "351d88ce97264e71b383e04119420b0d",
            "572b267f4fb7401aa79565ec22ce1304",
            "ce14b77c14a44b148ff99a7dbb69fce2",
            "4bd807c1e8644d3894c3307863e47f27",
            "da8aded2eb364778930c48fb56fd40af",
            "9407226512c446e899ce1baf2d8c0452",
            "d6a8f16670524121844999ebdb29f5de",
            "7c816db529f74e4d88f397e7dcef55d2",
            "c3dd3eba5ae64fe1a9a4581b10236530",
            "62f0ef2d80c445909b725e39e6a7da86",
            "eacf2a705e7b4db3b267c9b447085b91"
          ]
        },
        "id": "Sr8ES_fH3V3_",
        "outputId": "fd5df296-5af9-463f-c284-21b88040a6bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "351d88ce97264e71b383e04119420b0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/654 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = text_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        #max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        dataset_num_proc=2,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ok7iPI57vyL",
        "outputId": "c9cd0674-a2dd-46fe-89b7-2ee116943abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are given the equation:\n",
            "\n",
            "$$\n",
            "(x + 2)^2 = 0\n",
            "$$\n",
            "\n",
            "### Step 1: Take the square root of both sides\n",
            "\n",
            "$$\n",
            "\\sqrt{(x + 2)^2} = \\sqrt{0}\n",
            "$$\n",
            "\n",
            "$$\n",
            "|x + 2| = 0\n",
            "$$\n",
            "\n",
            "### Step 2: Solve the absolute value equation\n",
            "\n",
            "$$\n",
            "x + 2 = 0\n",
            "$$\n",
            "\n",
            "$$\n",
            "x = -2\n",
            "$$\n",
            "\n",
            "### âœ… Final Answer:\n",
            "\n",
            "$$\n",
            "\\boxed{-2}\n",
            "$$\n",
            "\n",
            "This is the only solution to the equation.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DHtl7JaG3iNd",
        "outputId": "929dc210-af71-463f-da3e-814053419590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 654 | Num Epochs = 1 | Total steps = 41\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [41/41 09:27, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.786200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.841600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.942600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.410400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.802500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.714200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.512900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.266900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.262400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.188300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.233200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.086800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.161100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.103800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.129500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.159300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.993700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.052900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.085200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.860100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.143300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.925700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.898200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.829600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.790100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.905400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.836300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.799500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.892700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.996000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.887600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.878100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.844700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc78znaO_jEi"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "class CustomNewlineStreamer(TextStreamer):\n",
        "    def __init__(self, tokenizer, skip_prompt=False, **decode_kwargs):\n",
        "        super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n",
        "\n",
        "    def put(self, value):\n",
        "        \"\"\"Override the put method to add newlines when '/' is encountered\"\"\"\n",
        "        if len(value.shape) > 1 and value.shape[0] > 1:\n",
        "            raise ValueError(\"TextStreamer only supports batch size 1\")\n",
        "        elif len(value.shape) > 1:\n",
        "            value = value[0]\n",
        "\n",
        "        if self.skip_prompt and self.next_tokens_are_prompt:\n",
        "            self.next_tokens_are_prompt = False\n",
        "            return\n",
        "\n",
        "        # Decode the tokens\n",
        "        text = self.tokenizer.decode(value, skip_special_tokens=True, **self.decode_kwargs)\n",
        "\n",
        "        if self.skip_prompt and not self.next_tokens_are_prompt:\n",
        "            # Remove the prompt part if this is the first chunk\n",
        "            if hasattr(self, 'prompt_length'):\n",
        "                text = text[self.prompt_length:]\n",
        "            self.skip_prompt = False\n",
        "\n",
        "        # Replace '/' with '/\\n' to add newline after each '/'\n",
        "        text = text.replace('/', '\\n')\n",
        "\n",
        "        # Print the text\n",
        "        print(text, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51K4JH8vVE_A",
        "outputId": "73bcb45e-0760-4af3-fa8c-25d89046a016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Il senso della vita Ã¨ il fine e l'opra \n",
            " che l'ingegno e l'amor ci fanno compiere, \n",
            " per raggiunger la somma beatitudine. \n",
            " Ãˆ come un'arco che mira a la luce, \n",
            " e ogni freccia che vola nel cielo \n",
            " Ã¨ un atto d'amor che cerca la sua fonte.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Qual Ã¨ il senso della vita?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = CustomNewlineStreamer(tokenizer, skip_prompt=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model from HF and inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.53.0.\n",
            "   \\\\   /|    NVIDIA RTX A1000 6GB Laptop GPU. Num GPUs = 1. Max memory: 5.801 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"Daniele/qweDante\",\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
