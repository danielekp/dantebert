{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dantebert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jdOMOIbghGd",
        "outputId": "cfa5ddd4-d229-4222-dca4-0887cec28b48"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "95c4afd61e374d3cacfd51baeed2712d",
            "dc8791a95381458c9e2a8862a56e4405",
            "664f7af0becd45118a7cdb4fec37739b",
            "b77b6587a122455b87de382f1e1a934f",
            "f416616e044a4d788072153e47b3e0d2",
            "c33643a14c904107ac956868ad6bea52"
          ]
        },
        "id": "9BxeCvi8gGvS",
        "outputId": "86d553a8-837c-4f0e-f04d-b215aeee056d"
      },
      "source": [
        "from transformers import AutoTokenizer, TextDataset,DataCollatorForLanguageModeling, Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocessing(path):\n",
        "\twith open(path, 'r') as f:\n",
        "\t\ttext = f.read().replace('\"',\"\").replace(\"«\",\"\").replace(\"«\",\"»\").replace(\"‘\",\"\").split('\\n\\n')\n",
        "\ttext = list(map(lambda x: \"<BOS>\"+x.replace(\"\\n\",\" \"), text))\n",
        "\treturn text\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-italian-embeddings\", max_lenght=256)\n",
        "model = AutoModelWithLMHead.from_pretrained(\"GroNLP/gpt2-small-italian-embeddings\")\n",
        "\n",
        "input_data = \"divina_commedia.txt\"\n",
        "\n",
        "text = preprocessing(input_data)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "train, eval = train_test_split(text, train_size=.9, random_state=2020)\n",
        "\n",
        "with open('train_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<EOS>\".join(train))\n",
        "\n",
        "with open('eval_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<EOS>\".join(eval))\n",
        "\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "     \n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset('train_tmp.txt','eval_tmp.txt',tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./dantebert\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=200, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=1500, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95c4afd61e374d3cacfd51baeed2712d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/135 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc8791a95381458c9e2a8862a56e4405",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/970 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "664f7af0becd45118a7cdb4fec37739b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/475k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77b6587a122455b87de382f1e1a934f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/280k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f416616e044a4d788072153e47b3e0d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c33643a14c904107ac956868ad6bea52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/427M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1143\n",
            "  Num Epochs = 200\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7200' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7200/7200 3:41:10, Epoch 200/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>10.290700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.929800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.066900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.611600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.824800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.545700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./dantebert/checkpoint-1500\n",
            "Configuration saved in ./dantebert/checkpoint-1500/config.json\n",
            "Model weights saved in ./dantebert/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to ./dantebert/checkpoint-3000\n",
            "Configuration saved in ./dantebert/checkpoint-3000/config.json\n",
            "Model weights saved in ./dantebert/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to ./dantebert/checkpoint-4500\n",
            "Configuration saved in ./dantebert/checkpoint-4500/config.json\n",
            "Model weights saved in ./dantebert/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to ./dantebert/checkpoint-6000\n",
            "Configuration saved in ./dantebert/checkpoint-6000/config.json\n",
            "Model weights saved in ./dantebert/checkpoint-6000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./dantebert\n",
            "Configuration saved in ./dantebert/config.json\n",
            "Model weights saved in ./dantebert/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c-rUMyCom2PI",
        "collapsed": true,
        "outputId": "88043234-c8d5-4042-95f3-3b50d3012aaa"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "dante = pipeline('text-generation',model='./dantebert', tokenizer='GroNLP/gpt2-small-italian-embeddings')\n",
        "dante(\"E colei che amai \")[0]['generated_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./dantebert/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"GroNLP/gpt2-small-italian-embeddings\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100,\n",
            "      \"no_repeat_ngram_size\": 4,\n",
            "      \"num_beams\": 10,\n",
            "      \"repetition_penalty\": 10.0,\n",
            "      \"temperature\": 2.0,\n",
            "      \"top_k\": 20,\n",
            "      \"top_p\": 0.9\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30003\n",
            "}\n",
            "\n",
            "loading configuration file ./dantebert/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"GroNLP/gpt2-small-italian-embeddings\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100,\n",
            "      \"no_repeat_ngram_size\": 4,\n",
            "      \"num_beams\": 10,\n",
            "      \"repetition_penalty\": 10.0,\n",
            "      \"temperature\": 2.0,\n",
            "      \"top_k\": 20,\n",
            "      \"top_p\": 0.9\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30003\n",
            "}\n",
            "\n",
            "loading weights file ./dantebert/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./dantebert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100,\n",
            "      \"no_repeat_ngram_size\": 4,\n",
            "      \"num_beams\": 10,\n",
            "      \"repetition_penalty\": 10.0,\n",
            "      \"temperature\": 2.0,\n",
            "      \"top_k\": 20,\n",
            "      \"top_p\": 0.9\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30001\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/61d34f8dfb7c3ffe977154bdb55f9c4710693f1b6a1906e9bb5137b3169cfce6.79a3bbc906c703094307f2010d6cb2883ae928674a9884228651b26ca388437c\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/4efdb93b73c15e8a27f37fc1da3f9f94699064cf159205af5872e0ec0213489e.6227370d56d17082437cb8711626b11e99ec5bbcc0c8e17f5d1760a61030894f\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/f86edc23a3474e3a7190f4522ce04a3b10f3a170e310221a62091f7ec217eaf7.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
            "loading file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/99f134cdd73cf165da27bb044dfbe8335fcc03aba30faeb9096ee0f4b531758e.3966d29321e63095f25361ba9a09e3db3ca4a3260118c28f30ff62177986e6be\n",
            "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100,\n",
            "      \"no_repeat_ngram_size\": 4,\n",
            "      \"num_beams\": 10,\n",
            "      \"repetition_penalty\": 10.0,\n",
            "      \"temperature\": 2.0,\n",
            "      \"top_k\": 20,\n",
            "      \"top_p\": 0.9\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30001\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/GroNLP/gpt2-small-italian-embeddings/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/614c5f30eb6281a7ed613ba272b87cf23f3a5f1c0c981ca6c3f1af45e1232b8c.d73fed30b04fd5115cc40d4fb714e82a4c1daa61c43fbaffb5e474e56152e799\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"data/hf/gpt2-small-italian-embeddings\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100,\n",
            "      \"no_repeat_ngram_size\": 4,\n",
            "      \"num_beams\": 10,\n",
            "      \"repetition_penalty\": 10.0,\n",
            "      \"temperature\": 2.0,\n",
            "      \"top_k\": 20,\n",
            "      \"top_p\": 0.9\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30001\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'E colei che amai uopo più si vuoli, d’altrui lume già bianche e fioche, quasi come piante novelle rinognate per l’acqua fosser pronte.e disse: Piglia quel seme a li occhi; volgi ’l viso, e fammi nota la larghezza di questa nutrice».S’el s’aunasse ancor tutta la gente che già, in su la fortunata terra di Puglia, fu del suo sangue'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lM_zB2exrVl",
        "outputId": "509fbba2-ef51-470e-ac9f-eb3c8d652780"
      },
      "source": [
        "!zip -r ./dantebert8epoc.zip ./dantebert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: dantebert/ (stored 0%)\n",
            "  adding: dantebert/checkpoint-3000/ (stored 0%)\n",
            "  adding: dantebert/checkpoint-3000/rng_state.pth (deflated 27%)\n",
            "  adding: dantebert/checkpoint-3000/optimizer.pt (deflated 8%)\n",
            "  adding: dantebert/checkpoint-3000/pytorch_model.bin (deflated 10%)\n",
            "  adding: dantebert/checkpoint-3000/trainer_state.json (deflated 65%)\n",
            "  adding: dantebert/checkpoint-3000/config.json (deflated 52%)\n",
            "  adding: dantebert/checkpoint-3000/training_args.bin (deflated 48%)\n",
            "  adding: dantebert/checkpoint-3000/scheduler.pt (deflated 49%)\n",
            "  adding: dantebert/checkpoint-6000/ (stored 0%)\n",
            "  adding: dantebert/checkpoint-6000/rng_state.pth (deflated 27%)\n",
            "  adding: dantebert/checkpoint-6000/optimizer.pt (deflated 9%)\n",
            "  adding: dantebert/checkpoint-6000/pytorch_model.bin (deflated 10%)\n",
            "  adding: dantebert/checkpoint-6000/trainer_state.json (deflated 72%)\n",
            "  adding: dantebert/checkpoint-6000/config.json (deflated 52%)\n",
            "  adding: dantebert/checkpoint-6000/training_args.bin (deflated 48%)\n",
            "  adding: dantebert/checkpoint-6000/scheduler.pt (deflated 49%)\n",
            "  adding: dantebert/pytorch_model.bin (deflated 10%)\n",
            "  adding: dantebert/runs/ (stored 0%)\n",
            "  adding: dantebert/runs/Oct11_13-48-51_eebfe4a669ff/ (stored 0%)\n",
            "  adding: dantebert/runs/Oct11_13-48-51_eebfe4a669ff/events.out.tfevents.1633960131.eebfe4a669ff.76.0 (deflated 58%)\n",
            "  adding: dantebert/runs/Oct11_13-48-51_eebfe4a669ff/1633960131.5584514/ (stored 0%)\n",
            "  adding: dantebert/runs/Oct11_13-48-51_eebfe4a669ff/1633960131.5584514/events.out.tfevents.1633960131.eebfe4a669ff.76.1 (deflated 62%)\n",
            "  adding: dantebert/checkpoint-4500/ (stored 0%)\n",
            "  adding: dantebert/checkpoint-4500/rng_state.pth (deflated 27%)\n",
            "  adding: dantebert/checkpoint-4500/optimizer.pt (deflated 9%)\n",
            "  adding: dantebert/checkpoint-4500/pytorch_model.bin (deflated 10%)\n",
            "  adding: dantebert/checkpoint-4500/trainer_state.json (deflated 69%)\n",
            "  adding: dantebert/checkpoint-4500/config.json (deflated 52%)\n",
            "  adding: dantebert/checkpoint-4500/training_args.bin (deflated 48%)\n",
            "  adding: dantebert/checkpoint-4500/scheduler.pt (deflated 49%)\n",
            "  adding: dantebert/config.json (deflated 52%)\n",
            "  adding: dantebert/training_args.bin (deflated 48%)\n",
            "  adding: dantebert/checkpoint-1500/ (stored 0%)\n",
            "  adding: dantebert/checkpoint-1500/rng_state.pth (deflated 27%)\n",
            "  adding: dantebert/checkpoint-1500/optimizer.pt (deflated 8%)\n",
            "  adding: dantebert/checkpoint-1500/pytorch_model.bin (deflated 10%)\n",
            "  adding: dantebert/checkpoint-1500/trainer_state.json (deflated 56%)\n",
            "  adding: dantebert/checkpoint-1500/config.json (deflated 52%)\n",
            "  adding: dantebert/checkpoint-1500/training_args.bin (deflated 48%)\n",
            "  adding: dantebert/checkpoint-1500/scheduler.pt (deflated 49%)\n"
          ]
        }
      ]
    }
  ]
}